Oreo, ahi, Ani, ore, aloe, Erie, area, idol, arena, odd, der, una, ural, oboe, B-team.

There, I’ve solved a third of tomorrow’s crossword puzzle for you. Or the next day’s. Pick a day.

There’s a visceral excitement in the potential offered by a blank crossword puzzle. As a self-contained unit, it’s seemingly as abstract as Sudoku or Latin Squares, a pure sequence of logic to be unraveled. But unlike so many puzzles, this one has relevancy, contains concrete associations that unfold as you progress. The objects, themes, and concepts that populate it arise from what you understand about facts, words, and ideas — in short, the real world.

A crossword puzzle calls upon knowledge in areas ranging from everyday items to titles of books and movies and plays you have to dredge out of your memory. It may include fanciful twists of wordplay that make you think hard to get to that aha when you solve it in the end. It combines logic, knowledge, and cleverness. And as you work your way through morning coffee and crossword, if all goes as hoped, you feel a little bit elevated: your mind clearer, sharper; you worked through — successfully — a cadre of recall and association and clever disguises communicated to you in hints by the editor. You feel you’ve taken yourself away from the prosaic for just a little while and engaged in something perhaps sublime in a small way, a pursuit that amplifies your intellect and reflects the culture that surrounds you, and which, most mornings, you are about to enter. All this is latent in the waiting puzzle.

And yet, often, there is none of this in reality. Often only mere echoes appear of what should be present. Have you noticed it? I can’t be the only one. More often than not, I make the effort more in hopes of the idea of enjoyment than actual enjoyment itself. There’s something missing. Each day’s puzzle feels rote yet abstract, spawning out of predictable word-banks and forming whimsical combinations of editorial expression which only nominally represent some cohesive theme. The majority of solutions are stock and store, commonly repeated one day to the next and almost certainly a significant portion used within any given week. Most remaining answers feel nonsensical, leveraged with heavy contortion to tie into their clues. Intentional variances in spelling and foreshortened clues leave only a semblance of relationship to a real-world vocabulary.

It seems unfair to complain — after all, there are only so many words that fit the specific patterns of tiles forming the standard 15×15 crossword puzzle. And so we see many of the same terms used over and over again, which is perhaps not surprising.  It’s hard to place blame when the cadence of the creation of these crosswords is daily and the subject matter is distinctly nontrivial.  There’s a natural upper limit to the words being used.

It’s worth noting that the current standard is not the only standard. Crosswords were published professionally in more variety in the past, in different forms and alternate layouts and sometimes even trading off a more sparsely populated puzzle for the ability to include different kinds of answers.

If you look back a few decades at the crosswords of the ’70s and earlier, you’ll see them. You can still find them now, actually; puzzles of all kinds are popular enough that if you scour puzzle and crossword sites, you’ll encounter crosswords large and small, modern and traditional, and with many new forms that never before existed. Look no further than Sporcle for some of the most quixotic of these. But they’ve largely disappeared off the primary drivers of the crossword: the daily newspapers, which, despite slowly disappearing over the precipice of traditional news media, maintain an impressive niche in some areas. Certainly if you search “crossword puzzle” most of the primary results will be from major news outlets. And they will almost all take “standard” modern form.

Is there a reason for this? I am no cruciverbapro; I do the crossword as a brief diversion and often find myself stymied on the Saturday puzzle (which tends to be most obfuscated). But it seems fairly plain to me that there is an expectation of densely-packed, relatively standardized crosswords, and that anything else is seen as amateurish, which is really a shame. There’s a lot to be offered by varying form and pattern, not the least of which is keeping the crossword in the realm of vocabulary rather than abstract patterning. A deviant crossword need not be a simple crossword (and a simple crossword need not be a bad thing as well, though perhaps for a different audience).

Maybe someone with better knowledge of crossword history can tell me if my assessment is accurate or if there are different driving causes behind this seeming standardization.

Regardless, the uniformity presented by this paradigm ensures that, being more densely packed, there is less wiggle room. Fewer ranges of words are available. I would claim, and with some fervor, that densely packed grids of predictable words prove no great enticement.

There’s an additional factor, too, that makes even the longer, better clues less rewarding: the lack of cultural cohesion.

In part, this is an unavoidable outcome of the (generally positive) shift in media consumption away from a few central producers, but the resulting fragmentation of culture means that what is commonplace and obvious to some proves incomprehensible to others, and vice versa. As recently as the ’90s, this was almost inconceivable. Think of Seinfeld: Most people in a certain population segment watched Seinfeld. Many of those people fell into the same grouping as those who would do the daily crossword.  Perhaps only the most highbrow of cruciverbalists would have skipped Seinfeld to the degree that a clue even referencing the show would be meaningless. In the ’90s, if you didn’t watch Seinfeld, you knew someone who did, and you probably knew the names of main characters and similar reference points.

Contrast Yellowstone, a reasonably popular show referenced fairly commonly in today’s crosswords (mostly for the names of one or two key actors). Yellowstone has had great viewership, and it’s likely to get better: several millions per episode, even over ten millions for some, and it’s possible it will grow as it is now reaching new platforms [1][2]. It’s not unreasonable to expect that many people are at least somewhat familiar with the show. But now look at Seinfeld: even early on, Seinfeld easily hit the teens and even twenty millions per episode, and later seasons regularly reached well into the thirty millions [3]. That’s a whole different scope of viewership. Combined with the more visual prominence of any major show at that time — you’d see it branded on anything and everything, and books, magazines, and newspapers all discussed them — it would be difficult to remain in complete ignorance of the show. This proves true for so many of the cultural artifacts in decades past, theater, books, movies, even news. There have always been different demographics and differing interests, but there were a lot more areas of common culture to point to where it was something that everyone knew.

Things that everyone knows are a lot harder to find now. Even a massively popular show on Netflix, Hulu, etc, is less familiar outside its key demographic. Then there’s more niche content: self-hosted podcasts, cultic YouTubers, infamous TikTok stars, high-brow anime, niche comic books. Each audience individually is relatively small and self-contained (with some exceptions, as they leak outside their spheres and into public consciousness), but each carves off its own unique slice of the cultural pie in viewership that often has increasingly little overlap with the next person’s. We all fall into this trap sometimes. The more we tell our friends about the shows and interests we know, that we like, the more we construct and dive into our own little bubbles of culture, the less anybody else’s makes any sense to us at all. It’s not just an unknown, its an incomprehensible, alien world.

So again, maybe it’s wrong to exactly blame the editors of crosswords for likewise occupying such bubbles — we all live in one, no matter how hard we try to avoid it. There’s just too much content to be familiar with even all of the more popular sub-genres. Yet again I also can’t help but feel that I’m not the only one left staring dumbfounded at a crossword clue that’s written like it’s complete common sense but means absolutely nothing to me, and then when I do finally manage to solve it — usually through pattern-matching rather than getting the reference — I have to go look up the answer itself, just to learn that it’s some minor actor, some cryptic reference to a show, or some localized slang that I’ve somehow managed to completely avoid discovering until now.

I’d like to think I don’t totally live under a rock. I try to maintain a fairly broad variety of friends and stretch my conversations to at least comprehension of the commonly-viewed trends out there. But the experience of the daily crossword tends to set me straight in that regard — either the editors live in a completely isolated bubble of culture, or I do.

Probably both.

One more complaint: Many of the words in a modern daily crossword are not English. French, German, and Spanish rarely miss a day. They are tantalizing bait for the editor — the most commonly used words taken from these wells are short and vowelful, the ideal supplement to the increasingly congealed form of the daily crossword.

All these failings and limitations were taken to a pinnacle in one crossword that seemed to make very little sense to me even after working out about half the clues, and this was, I discovered, because many of the words were not words, but words with a seemingly random letter appended to the end of each of them, which, when flipped to form the crosswise answer, was a phrase related to the nominal theme of the puzzle.

Here, finally, was a crossword so tortured, clues so divergent from their answers, that a good portion of the puzzles entries ceased to be actual comprehensible words. There are more common examples of this in words being truncated or flipped, but to my mind this falls into a different, if equally frustrating, category since those at least follow a pattern, such as swapping all ‘s’s for ‘c’s and so on.

Here, on the other hand, the clues of the crossword were not even themselves hints of how to manipulate a given word to end up with some other combination of word. It was merely words that were sacrificed on the altar of the greater answer.

I’m not going to name or reference the origin of this puzzle as that would be inappropriate, and it wouldn’t be too hard to figure out if someone really cared to, but this culminating experience has really taken my enjoyment down a peg on crosswords. I still dabble some days, but my heart’s not in it.

As a side note, it’s sometimes entertaining to make your own crossword — there are some good tools out there, and if you’re willing to put a little time and brainpower into it, there’s some reward in crafting a puzzle that’s built just for you. But it’s not a brief process, at least not if you want a unique result, and even with those modern tools, how much time are you going to put into creating something when it’s not your job, not even a principal hobby, barely a minor hobby, more a daily indulgence.

This made me think about what this change of the crossword really meant, and I started to think about the different types of crossword, and the sometimes subtle dividing lines that separate what my definition of a “good crossword” is from the esoteric, abstracted puzzle within a puzzle that so repelled me.

In fact, the conversations that I’ve seen online about crosswords, by those who are truly enthusiasts, seem to revel in the level of abstraction and the desire to perform this word mapping, whereas I suspect — but can’t prove — that most of the everyday people who would have previously done a crossword no longer do. I know of a few examples outside myself where this is true, but that’s hardly a basis by which to judge.

Instead it pays to approach this question from another angle. A useful analogy here might be chess.

I don’t know how many people remember this phenomenon, and in fact it was mostly gone by the time when I was first looking at newspapers for anything but the comics, but it used to be very common for most major newspapers and a lot of minor ones to include chess problems, boards set up in a certain way you could attempt to solve, much as you would with a crossword, or demonstrations of various matches that you could follow.

But these are gone away — and why?

Have people lost interest in chess? You could easily assume so: Certainly, the Luddites among us, in whose ranks I sometimes fall, would say that people have lost interest in the “traditional entertainments” like puzzles and more thoughtful games; people are interested in other things, absorbed by media — and that’s true, and yet chess.com, to cite just one example, is incredibly popular, receiving hundreds of millions of visits a month and ranking within the top several hundred sites overall (even higher in the US) [4][5]. Chess remains well-liked among many segments of the market (many of which overlap with the newspaper-consuming market). A recent chess championship famously took the limelight, for slightly more unpleasant reasons than pure enjoyment of the game, but the point remains that the “average person” is not the stranger to chess that more closeted minds would assume. A lot of people play chess, even if just sometimes. I have even met rehabilitated prisoners who used their time in prison to refine their tactics in the game.

So why isn’t chess in most papers? You won’t find chess puzzles even in most online versions of various daily papers.

My conclusion, with which I hope you will either agree or present me with a counterargument for, is that it became esoteric.

The people who remain interested in chess don’t think of it as an everyday activity, they think of it as a rarefied activity meant to stretch their brains and occupy their time with something a bit loftier (even if they would never use these words to describe it).

Of all the players I’ve ever met at this point, and I’ve met a fair number, they tend to fall into one of two groups: The first group, the dabblers, play on occasion. Like myself, they enjoy chess as a concept, appreciate the chance to think and examine the board and plot out strategies and test them against a friend or nemesis. And they’ll enjoy playing, until they run across someone from the second category, the avid hobbyists who really like chess, and the first guy will get completely steamrolled.

Because you get really good at chess for the same reasons that a computer is good at chess. One, you’ve played enough to memorize the standard openings and endings, the correct ways to begin and end a game and respond to your partner by selecting the appropriate sequence, maybe slightly modifying it if needed, which is not often. And then, two, by process of playing hundreds or thousands or tens of thousands of games, perhaps supplemented by reading chess theory, you understand the construct of the board such that you can intuitively tell what is likely to be the next optimal move in this scenario.

Success here comes somewhat from strategic thinking, but much more so from an intense familiarity with the peculiars of the internal mechanisms of the game, the panoramic comprehension of likely moves to the extent that you know what the opposing player is likely to do next and therefore which move you should make. This is a barrier I cannot cross, a barrier a lot of people cannot cross, or don’t want to, either through lack of time, lack of interest, or lack of raw brain power in some cases, honestly — that’s partly my limitation.

I suppose most of us could get past that hump if we were willing to spend the time memorizing patterns and strategies, but it would be time consuming, and a challenge, and the frustration along the way just proves not worth the while. And so the lingering popularity of chess is less and less with those in the first category, and much more in the second category.

And that is why, I’m almost sure of it, chess is no longer in the paper. It’s no longer for the layman who wishes to improve himself a little bit that day. It’s not for the average Joe who considers himself a bit more white-collar than he is blue-collar, for the person who is maybe a tradesman but keenly intelligent. It’s for someone — in any rung of society, doctor, lawyer, tradesman, or convict — who is really into the esoteric value of chess, the abstract permutations of what today’s game might prove to be.

Casual, largely amateurish play (from the ranking’s point of view, anyway) is just not part of the chess experience anymore. Chess, by and large, has gone past those bounds and I wonder if crosswords will, too, for the same reason.

Putting chess aside again, the particular crossword that I mentioned earlier, and those of its ilk, that have been building within me this head of steam that came to a front now took me to want to represent in more concrete terms my dismay and disdain. And so I took to ChatGPT — not having the wherewithal or the desire to attempt to it myself — to write out a little story expressing the contrasts and frustrations in this bifurcation of the puzzling world, and after just a couple iterations, it came up with something, and I think it’s pretty good. And here it is.
 

    In the chrome-touched twilight of 2073, Gerald J. Fitzsimmons III, of the esteemed crossword-conjuring Fitzsimmons lineage, sat in his penthouse office overlooking the cityscape of New York. His eyes were fixed on the 15×15 grid spread out on his sleek, holographic desk.

    He traced the glow of the clue for 7-Across. A grin touched his lips. The clue: ‘Neoplasmic metaphor in the ironic narrative of Zorgon-5’s sixth quadrant (3rd revolution)’. An artful nod to the quantum fiction genre, a casual yet profound exploration of cosmic uncertainty. He could already visualize the threads of discussion this would spur in the crossword forums, the potential for scholarly exploration.

    There was a fervent joy in it. For Gerald, it was the delicious thrill of unraveling the cosmos and stitching it into a tiny grid, a symphony of knowledge and culture. It mattered little to him who could, or would, attempt the solution. He was painting with a palette of words and wisdom, and each clue was a stroke on this canvas of his design.

    “Now that’s a crossword!” he murmured to himself, basking in the intellectual reverberation of his creation.

    Meanwhile, several miles away, under the fluorescent lighting of his Toledo auto shop, Randy struggled with the same crossword. The clue, that damned clue: ‘Neoplasmic metaphor in the ironic narrative of Zorgon-5’s sixth quadrant (3rd revolution)’. It echoed in his mind as he cleaned his grease-blackened hands.

    He glared at the nonsensical jumble of words, the labyrinthine puzzle as inaccessible as the wealth of the customers whose luxury electric vehicles he serviced. He missed the old crosswords, their worldly clues hinting at familiar corners of life, the satisfaction of ink filling squares, the joy of catching a clever play on words.

    Randy tossed the newspaper onto his workbench, the page crumpling under the weight of the spanners. “Jes’ more of that high-minded nonsense,” he grumbled, his hands returning to the stubborn nuts and bolts of the physical world in front of him. His grip on the wrench felt real, solid, unlike the abstract gibberish the world seemed to be obsessing over.

    And so, as the city spun on in its chaotic dance, two men, worlds apart, shared a moment of connection over a grid of black and white squares. One, lost in the delight of complex creation, and the other, recoiling from its esoteric excesses. In this peculiar dance, the humble crossword had taken on a life of its own, its 15×15 universe birthing a new paradox, simultaneously celebrated and detested. And in that, it perhaps became the most accurate mirror of its time.
	
	It’s not really surprising, but I’ve seen — like everybody else — the concern over AI rising in proportion to the ascendance of AI itself. It’s a concern not without cause, but often it is presented without reason. That is to say, the dramatic and very tangible effects of AI cause visceral fear, and this fear drives arguments which do not always reflect the most well-reasoned possibilities. It’s a level of technological fear possibly not seen to this extent since the original fears of automation in the ’70s.

Those fears were genuine then, and they are genuine now.

But the legitimacy of the fear of change and what disruptions it may cause to life and livelihood do not justify any and all arguments that we might choose to make against AI. If anything, we should take stock of exactly where the danger does and does not lie, build for ourselves parameters of fear, a kind of foundation upon which to build a credible argument in those areas we want to change.

Concern that generative AI may cause mass layoffs in the near future? Reasonable (in fact it has already begun, though the decisions for those layoffs often reflect serious misunderstanding).

Concern that generative AI is evil, cruel, and should be governed out of existence? Unreasonable, and not based on critical thinking if we pause and evaluate.

Concern that generative AI may revolt and take over the world? Unreasonable…for now, but we should check back on this in a few years, as the emergent properties of AI become more quixotic. But it’s the human use of AI that is worrisome, not a sentient AI, not yet at any rate, and it’s there which we must draw a dividing line and marshal our arguments.

Yes, there is grave danger in the misuse of AI. Yes, it can be used for harmful and destructive means (which, certainly, is a first for any technology ever /s). Yes, there is a looming pitfall ahead which may cause us to easily fall into a hedonistic, meaningless fantasy world in which nothing of real value is created, and endless remixes of artificial realities play on repeat in our minds.

However, used thoughtfully, AI can supplement, rather than supplant, the human effort.

One example: An author who has created a beloved fantasy world over the course of several equally beloved books. These books, which are the product of years of effort, represent the tip of the iceberg of the creative landscape sitting largely dormant in the mind of the author, who cannot possibly explore this landscape via the written word on anywhere near the scale on which he can imagine it. Exciting and innovative material that a multitude of fans positively slavers to read and explore remains locked up in the author’s head. Probably he will never have time to write them all — without generative AI.  But with generative AI he may create far more books exploring this world for his fanbase, using AI to flesh out short sketches of material, bare guidelines that can be turned into fully-fledged works in a fraction of the time it would take to write every word himself. These products remain the author’s material (in principle; the legality is more complicated), they are guided by his input, they expound his own ideas and build a world that he alone thought of, and not the AI.

However, it is true that these new books will not be literary masterpieces. Probably the original books were not either — true written works of art are rare. But high-quality writing, just a step or two below the level of the masterpiece, are an achievement, too, and all the more so when they are turned out at a truly inspiring rate, chock full of interesting material with fans waiting ready to digest it.

It’s important to recognize that only humans, at this point, may create truly original material or generate new and original works of art (of any kind). But there is value, sometimes, in being able to produce more of something of average quality than less of something in slightly better quality. Fantasy novels are one example; others can include levels or characters in a game, endless catered background music, any creative work you might think of (a blog post, perhaps?). None of this takes away the need for true human endeavors of originality and inspiration, and we should recognize that, too, but that doesn’t mean that there’s not something wonderful in what we can do with AI if we do it right.

Well, what is the difference? What makes one AI-generated book good and another wasted content?

Here is one case where the end justifies the means. Amazon is being flooded with AI-generated e-books. Most of these, it seems, are created by “authors” taking advantage of the virtues, as such, of generative AI. Are any of them good? Probably. I haven’t looked to check, and I suspect most other readers haven’t, either. If we run across a book we like we might not know it was generated by AI, or we might suspect it by some quirk or subpar direction of the writing — but who knows? Regardless, most of this drivel passes directly from the maw of OpenAI and others to the waiting chasm of Amazon’s endless list of $0.99 e-books which sit largely unread. There is no audience, even if one occasionally gets picked up; there is no benefit, other than to Amazon and the occasionally lucky generator of the text.

The author, however, who has written — with the aid of AI — a purposeful book, based on an idea of something interesting to write, rather than to take advantage of a trend, launched with a spark of creativity, who has generated interest, gained an audience, finally produced a work which is received with pleasure by at least some niche audience — this author has used generative AI well, whether or not I or any particular individual thinks the end result is “good quality writing.” The point is that it exists, and has been appreciated. That’s what matters. It originated in the author’s mind, not ChatGPT’s, and whether the assistance given by AI was 5% or 95%, the result was an original idea brought to fruition and accepted by one or more willing and satisfied readers.

This justification, this differentiation, of the use of AI is a microcosm of the entire “battle,” for lack of a better term. There are so many dangers on this path we’ve started on, but there is so much promise, too.

And, most importantly, it is very, very rare and very, very challenging — if not entirely impossible — to put the genie back in the bottle. We may already be past the point of critical mass. We might not be able to foresee or avert all the pitfalls that lie ahead. But if we approach the issue cognizantly, and take to it not as fear-mongers but as arbiters of good judgement, bringing practical and useful and balanced standards, easily understood and easily accepted, then we may just be able to help guide how AI is used, and set the example ourselves.

Part one of an unknown length series

If you stumbled onto this article and expect to learn something of AI in a traditional fashion, you might spend your time better elsewhere (even if that’s another article on this blog).

Usually if I’m going to bother writing and posting a blog item, I usually want it to be worth reading to all within a given audience — to provide some (I think) interesting perspective or observation. This post doesn’t do that. This post is not deeply contemplative, doesn’t teach anything directly practical, or even offer a thoughtful opinion. This blog post just tracks a record of something I’ve done — one step in a very long process.

In the long process of learning AI, I am trying to “meet myself in the middle” as it were. Studying the fundamentals of ML and DS is critical, but it is a far cry from the day-to-day growth of the AI industry. And working with practical AI tools may be both fun and productive, but does little to explain the inner workings. To understand as best as possible, and in a practical way, I am hoping to keep working my way up from the fundament, and chipping my way down from the paramounts, until I have as complete an understanding as is possible without obsessing myself with it to the exclusion of all else, and without being a math whiz.

I’m writing this post mainly for my own edification and reference, but I’m publishing it on the chance it might happen to prove useful to another. In any case, documenting your journey aids in personal accountability so they say.

And if you have interest in seeing this thing through, then I apologize in advance for the lack of brevity inherent both to my style and to the medium — but not for the process of learning, which is best when slightly ponderous, so long as the ponderousness does not hide sloppiness.

Onward.
Step 1: Get something running

Since I’m approaching AI learning from top-down and vice versa, hoping to meet somewhere in the middle, I wanted to get the “feel” first of running one of the major AI tools locally — StableDiffusion seemed like the obvious choice, with some historicity already and with a number of readily available approaches for getting started.

I worked through the diffusers python package using their tutorial, mostly: https://github.com/huggingface/diffusers/tree/a28acb5dcc899b363c9dd1c8642cddc9b301cd9d
(It’s since been updated somewhat, you can check that out too).

I recorded my process as I went along (actually, that’s kind of a lie; the first time I recorded nothing, but then I switched machines and tried it again, and recorded that) so that I could end up with a kind of “foolproof” plan for getting set up locally in a way that avoided the somewhat generic and nebulous explanations often found in these tutorials, which assume, reasonably enough, that you either have a decent background in AI if you’re following along, or will just be reading and not trying to implement it yourself.

Well, I’m somewhere in between all that, and I found many supposed guides difficult, at least in starting. So I “boiled down” a few of those into something digestible and repeatable I could use on my system, and in the process was able to ruminate a bit better on what it is I was actually doing.

I went through the process a handful of times to make sure I could nail down the exact steps. This walkthrough was recorded on a Win10 desktop with 32GB ram, but I completed these same steps on a Macbook with 16GB ram, the only difference being I used miniconda instead of pip as the package manager.

The process I ended up with proceeds as follows:

    Install python (python3)
    Install pip
    Install pytorch and associated packages: pip install torch torchvision torchaudio
        You might need to install them directly from source:
            pip3 install –pre torch -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html
            pip3 install –pre torch -f https://download.pytorch.org/whl/nightly/cpu/torchvision.html
            pip3 install –pre torch -f https://download.pytorch.org/whl/nightly/cpu/torchaudio.html
        You might also need to install additional dependencies if the output from pip tells you it didn’t resolve these.
    Install diffusers and related boilerplate packages: pip install diffusers transformers accelerate
    Set up large file support on git if it’s not already: git lfs install
    Clone the stablediffusion repo: git clone https://huggingface.co/runwayml/stable-diffusion-v1-5 (there are newer ones, may require further steps).
    Create a file. The file I used leveraged some of the examples in the tutorial to run locally, use CPU instead of GPU, and limit memory usage.

I found that this process was repeatable (I ran through it a couple of times) and that the actual image generation was relatively speedy — the training already having been completed, leveraging a pre-trained model.

I created a reference for myself, which only a couple of weeks later is probably already outdated, given the pace of things.

Step 2: Understand diffusers

“Great, you’ve used diffusers to run StableDiffusion yourself, and it took all of five lines of code, because everyone did the work for you!” — so I hear myself thinking. I am glad I was able to load and run everything on my machine; it’s cool, interesting, useful, and I did learn something — but can I really claim any understanding?

Well, I can dig into the usage of the diffusers library, start tweaking my code and reading the documentation, but I think before I do that, there’s some utility in actually understanding a bit more of how this thing’s built!

So, only a click or two will land you on https://huggingface.co/blog/annotated-diffusion (at least at time of writing, I hope it sticks around, at least as long as this is relevant), and you’ll learn how to implement diffusers. Cool, actually building it from scratch! — well, not from scratch, since you’ll be using pytorch, numpy, Pillow, tensors, matplotlib, and others, but if you follow step by step you’ll at least gain something akin to comprehension of the purpose of the papers that form the foundation of this thing — and have a clue what your code is doing.

It’s something I had to struggle to keep in mind while working through this breakdown, and which I think afflicts us easily when we stretch ourselves on a new endeavor — the goal is not total and unerring comprehension. You’re not aiming to be an expert; and even if you are, eventually, you’re not aiming to be one right now, at the conclusion of this single tutorial — that would be foolhardy indeed.

As one of my compsci professors once told me, and it stuck: “If you understand 50% of a paper, that’s good enough.”

Few can hope to be functional experts in more than one niche in an industry, and even if you have solid comprehension of the breadth of an area of research, that won’t help you in understanding the details any particular branch which you’ve never spent significant time studying, and which isn’t your main focus.

But that’s not a problem — reading comprehension in research is its own skill. Look at a formula you’ve never seen before, representing a concept you’re encountering for the first time, and be able to get what it’s trying to say, rather than the exact mathematical construction, the purpose of each variable and function. If you recognize an inequality, pick out a ratio and note that as it increases, something else scales logarithmically, then you’re following. If you can scan the methodology section of a paper and understand what the main pitfalls are and the source of those problems, then you don’t need to be able to replicate or even understand each step yourself in order to glean useful comprehension from the paper as a whole.

The takeaway for this and other purposes like it is that you shouldn’t, in most cases, be trying to do the author’s work for them, but rather be trying to replicate the actual comprehension process that got them there — just like when you learn math or physics, no one expects you to imagine the formula for Newton’s law of cooling on your own, but as you learn it, you become able to derive it, you follow the reasoning and in doing so, understand the process and the purpose.

I found great challenge in this particular tutorial / code walkthrough because my math skills are just below the level to totally grasp the nuances of what makes the denoising methodology they implement work so darn well. I could “get the idea” but I couldn’t track the details of the Gaussian noising, or even always spot the purpose for a particular line of code (Why is it necessary to rearrange the shape of the matrix here? Why couldn’t we sample as is?). It’s not only frustrating, it feels very defeating to the purpose itself — how am I really learning anything if I can’t reproduce it myself? I almost stopped twice partway through.

But then I remembered I am learning — a lot, for that matter. I’m learning what is happening even if I can’t personally reconstruct the details of why. It’s not my job to know the why — certainly at least not here and now. My job is to learn, to follow along as best I can, focus on the meaning of the code, absorb the goal of each step and the approximate method by which it’s accomplished and what intrinsically makes it work. I trust that the details that I need to know, I will know, in time. If I push it now, it’ll be that much hard to learn, and I’ll just end up hating it to boot. But if I get the sense of things now, along with the sense of accomplishment, then I’ll have a successful basis from which to learn more.

This is all pretty fundamental stuff when it comes to learning, but it’s easy to forget, especially for me, and especially when I get wrapped up in a length learning process which comes to fruition only through the more hand-on process of “let’s get something working.”

I’m happy to say I did get it working, and do basically understand it — and I have three more tabs open already with further papers and tutorials that I’m looking forward to following along, encourages and not dispirited by what I’ve already seen, learned, and accomplished.

Well, enough of that tangent. Here was my process:

In this case I mostly was able to stick to the script. All my setup and packages were already in place from the last step so there wasn’t nearly as much fidgeting with my environment.

The only real difference was that I ran in a command-line environment, not in jupyter or similar, so I had to make a few changes to compensate for that:

    Comment out %matplotlib inline (wasn’t using inline output anyway).
    Install torchvision directly, and add an import for it
        pip3 install torchvision -f https://download.pytorch.org/whl/torch_stable.html
    Add an import for ImageShow from PIL to display images at the steps, then add lines:
        ImageShow.show(i), or
        i.save(“out_noisy.jpg”), etc.
    Add show() to plots where needed
    Enforce caching on the dataset load, which did not occur automatically. Interestingly, once I included this flag the first time I ran it, it would no longer run on further instances until I turned it off — I can speculate on what blocked the caching and made it hang (OS quirk?) but I don’t know, and once I removed the flag, it worked smoothly and was able to draw from the cache.
    Lastly, I encountered the strangest difference in behavior where the script never triggered the image save step. I had to modify the code as written to use a different save counter, and also had to add a step to manually concatenate the sublists of tensor-y images or torch.cat would fail with an argument exception — I don’t know how the structure differed from that which was expected, given I rewrote / copied most of the code verbatim, but somehow the functionality differed sufficiently that that part needed rewriting. Maybe I did something wrong?

Most of this process was by trial and error, which is astonishingly shameful to my skills, given that the code was offered up on a platter. A lot of the debugging process relied on comparing the intended functionality with the results the code turned out. All of it was theoretically “pointless.” In the end you’re not really building anything so much as demonstrating an implementation. But it was still a great way to learn! The abstract math made a lot more sense when you struggle to figure out the way the tensors are transformed in each step.

Side effects of some of the issues I ran into: I now have a better sense of torch and even parts of python itself (especially pdb, the debugger!) than otherwise I would if everything had proceeded smoothly. Coming from a background of Ruby and JS, and before that PHP, Python has always been like a vaguely foreign but eerily familiar tongue to me — reasonably easy to understand until it breaks out some completely incomprehensible line that might as well be gibberish. But that’s where the learning starts!

Well, one step farther along in this somewhat ill-defined journey. I can’t say I possess any new concretely applicable skill, but I feel more informed as a whole, certainly than before I started these implementations.

Onward!

Soon to come: Step 3, and who knows what else.

The future of coding itself is at stake seems to be the message of many articles and discussions in recent months. Certainly anyone whose career lies in software development and related fields has reason to ponder the significant leaps forward of artificial intelligence in this area. From GitHub’s CoPilot as a VS plugin to prodding ChatGPT to churn out whole components’ worth of code, the industry seems at a crossroads. Can software development as a career survive the ascendance of AI? Many of us are prompted by our own friends and colleagues to contemplate this question with increasing unease.

Predicting the demise of programming (in one form or another) has recurred as a theme for decades (and many other industries besides, but that’s a separate story). It’s perhaps a justifiable target: the bones of software development are easily abstracted — break a goal down into steps, map the structures and sequences necessary to complete those steps, then turn that conceptual algorithm into written code. While practice layers many complexities on top of these basics, the fundamental processes of software development seem ideally suited to automation — well defined and repeatable steps being the bread and butter of any automation.

Until recently, any such automation was limited due to its intrinsic rigidity: translating well-known and well-defined patterns into code is easy enough, but any small deviation leaves the automation useless at best. Previous generations of AI could sometimes recognize within a problem its steps, its components, but could not bridge the gap from conceptual steps into contextually accurate code. It took enormous sets of data, and the advanced models that can process and predict that data expediently, to turn theory into reality, and pose a true threat of automation to the world of software development.

The endless cascade not only of news items but of actual AI-generated code now prominently appearing in applications threatens to turn this vague specter of automation into impending reality.

There are reasons to think this analysis is correct, and reasons to think it is ridiculous. Neither, in reality, is a reason to worry — in fact, quite the opposite.

The Good, the Bad, and the Ugly

Take a gander, if you haven’t yet, at the kind of code that ChatGPT (and other tools — don’t want to harp on any one in particular) turns out. Typically when we go to an AI and get it to spit out some code, we are “playing around.” We want to see what it can do. So we offer it the first thing that pops into our minds, and voila, it succeeds…

https://i.redd.it/w6s0brc2dp7a1.jpg -- ChatGPT turning out an "evil" hello world function.

(Not my code – and this is unlikely to delete anything as written).

Well, that’s impressive code for such a short prompt, but it’s also not very surprising. The code in this sample represents some of the most basic scenarios possible in programming. Is ChatGPT’s interpretation of “evil function” interesting? Absolutely. But we should be shocked if an AI of this nature actually failed to write a Hello World function. After all, you could make a chatbot in two minutes from scratch that could do the same. What’s most pertinent here is ChatGPT’s natural language modeling, and what it makes of the quirk in the request — not the code itself.

If we want to see what ChatGPT can do at its limits, we can ask it something a bit more out of left field.  One such “challenge” is to give it a more complex or rare problem; another is to query it for a solution in a space where there is likely a relative minimum of source content.

So that’s what I tried, asking ChatGPT to write modeling script for a raytraced champagne flute (using POV-Ray, a long-time raytracing software, so as to give ChatGPT a fighting chance).

It actually almost kind of worked. ChatGPT clearly “understood” the goal of the request, and even framed the response in the context of how it tried to build the response:

Unfortunately, the code did not compile. Even after a few trial-and-error adjustments, the scene that finally rendered was definitely not a champagne flute, no matter how much ChatGTP insisted it was:

Even more interesting, to my mind, was the persistence of one particular bug which no amount of bartering back and forth with ChatGPT about how the script should be written managed to solve. It was clear that wherever and however it was sourcing its understanding of constructive geometry, it was stuck with a fundamental misconception of how that code works. Possibly it was even conflating POV-Ray to some degree with another form of scripting, given the limited number of examples it was likely to have encountered in this space.

It’s pertinent here to note also that ChatGPT is a) episodic, and b) unreasoning. What this means in practice is that ChatGPT attempts to build a response that most closely adheres to what it expects matches what you’re looking for. (You can hold an ongoing conversation with ChatGPT because it replays the chat history into its input for each subsequent response).

Ok, so what’s the lesson here so far? Even the greatest skeptic ought to admit it doesn’t really prove much to chuck a highly niche code request at an AI and then balk when its answer is imperfect.

But we’re evincing here an obvious but easy to forget fundament of generative AI: unlike with humans, the whole is not more than the sum of its parts. More on this shortly.

To create a contrast in what ChatGPT could handle, I switched to a more common programming paradigm — asking ChatGPT to create a simple text input in HTML with some styling. It was able to come up also with a pretty functional bit of javascript to tie into the text box to make it simulate a terminal. It still took some tweaking, but it was much closer to something usable than the previous attempts at generating raytrace script:

Taken together, this failure, and subsequent success of sorts, points to the limitations of what an AI can currently accomplish, as well as by contrast the uses it can serve well.

It’s important to point out that these operations were on the standard ChatGPT model, whereas models trained on a huge set of common programming building blocks will turn out improved results (some already do — like text-davinci-00x). But any and all such AIs in this style necessarily can produce only interpreted solutions to this same realm of problems. ChatGPT can mimic reasoning by virtue of having ingested and processed countless reasoned arguments, and yet cannot actually recognize the inherent logic or fallacies in the conversation. Likewise a model for programming can not recognize its own inadequacies unless explicitly trained to do so — even if it does a great job of making it seem like it does this by default. The obvious lack of contextual comprehension speaks for itself. An AI can turn out a fancy piece of code, but it’s equally likely (at this juncture) to turn out uniquely elegant nonsense.

The outcome of this gap between trained models and true comprehension leaves two major aspects of programming uncovered by even the most advanced AI, and leaves humans as highly relevant for anytime in the foreseeable future.
AI Hates a Mirror

The first of these areas is the most obviously limited by training data — AI cannot innovate. It can occasionally appear to innovate; you can ask an AI to build an original JS framework for front-end development, focusing on minimal set up time — and it will deliver. But what is delivered will not be novel. It might prove useful, and it might give a human developer new insights, but the initial result will be fundamentally dependent on the AI’s training. In other words, you cannot find a new axis, a new dimension, via linearly dependent vectors, no matter how numerous they may be.

For this reason, the development of anything truly novel will, for as long as the current approach to AI remains the focus, be dependent on the human developer. Like any great innovation, a technological disruption generally comes from striking out in a new and unexpected way — exactly the opposite of what trained AIs accomplish; in fact the very opposite of their purpose, which is after all to adapt to known data!

This limitation mirrors what we see more obviously with AI in the realm of art — the artifacts, the images, that are turned out by AI can be beautiful, stunning; may appear imaginative – even inspire our own new artworks by giving us mental fodder. As graphic art, it excels.

But as fine art, it fails.

Such artifacts say nothing of who and what we are, as art so fundamentally does, being an organic expression of our own experiences. AI-generated art can speak to our souls only by reflection of what other art has already done in the past, or by total accident. A novel work of art is left, in essence, to the viewer as an exercise, so to speak. It remains for the human to conceive something new, and perhaps then leave for the AI new data to ingest in the form of an original art style or transformative piece.

What is an obvious fact in art transfers true in software engineering. In order for programming as a field to avoid a horrible kind of stagnation, it will be necessary for humans to remain at the wheel — at least for now.

But there is a second reason why humans are needed for programming, beyond innovation, and it is even more fundamental to the way humans operate compared to the way an AI operates.
The Best Worst Coder

A human, in processing their own environment and experiences, in knowing what those mean, possesses an instinctual (albeit instinct that is learned) sense of sensibility — i.e. “does this make sense.” A human with even a smidge of background in programming can eye a piece of code and tell you whether it makes reasonable sense or is nonsense. Even if they fail to point out some lexical error or missing statement, they can identify the rationality of the sequence.

This key ability goes beyond simply avoiding the absurd if humorous mistakes that an AI may churn out. It is self-reinforcing — because a human, understanding what code means, can see if it will actually work, if that code can be trusted and used as a component in something bigger. Would you trust an AI that can never understand its own code to generate a new module for your application? Or would you rather have a flawed human being, whom, having been corrected, can reason about that error, and immediately use that reasoning to write better code?

Software developers — and other engineers or builders of any kind — rely on each other to have a sense of whether an output is sound, whether it is “good to go.” If it is, it can be used with other outputs to build something larger. Whether those outputs are coding libraries, conceptual protocols, or steel girders, it is the reliance on the components that makes any kind of building at scale plausible.

While engineering of course implements checks and evaluations, and for software, all code generally will go through a review process, the most hardened of reviewers will find themselves begin to be disillusioned if a sizable portion of the code coming down the pipe includes obvious jibberish that cannot be excised no matter how emphatically the error is explained to the one writing it.

To make a long discourse short, comprehension is elementary. And comprehension, for the time being, requires the human touch.

Nowhere in all this fault-finding should the inference be taken that AI has no place in programming.  If we are realistic and honest about both the limitations and strengths of AI, it can be used to incalculable benefit. As a labor-saving service AI is incomparable. As a gateway to exploration, discussion, and discovery, we’ve barely begun to tap the potential of the AIs we already have, to say nothing of future developments. AI cannot truly create, only mimic — for the time being — but that does not present much of a putdown.

An artificial intelligence is an uncomprehending behemoth — but it is a majestic behemoth, one that can thresh the seeds of creativity with unmatched power.

Since we’ve mostly been ragging on the ineptitude of AI-as-coder so far, before we close let’s take a very brief survey of the startlingly impressive tools and abilities AI can already offer in software development:
1. Code design and generation

We’ve seen this one already. What we haven’t demonstrated is how effortlessly (it would seem) ChatGPT and others can generate not only code but even discuss the architecture and implementation for that code (what’s shown below is a fragment of a longer conversation).

And so on.

It’s still not a fully formed response — and you still wouldn’t want it to write the whole application itself. But it’s useful, and time-saving, especially when prototyping!
2. A second pair of eyes

CoPilot, GitHub’s code-completion AI, took the spotlight quickly, as it not only blew users away with its abilities, but also with its willingness to make use of code from other people’s private repositories. CoPilot already has multiple alternatives, both open-source and proprietary, and the world of software is working on all burners to move this kind of AI towards even greater effectiveness.

3. The one who debugs when you don’t want to

Many software developers say that debugging a piece of code requires 20% more brainpower than writing it. It’s certainly tedious, time consuming, and often ends in the realization that the deeply complex bug you’ve been pursuing was in fact a misplaced quotation mark. AI does an excellent job relieving you of this burden, and a number of AI-based tools have leapt forward to be your go-to debugger. This is, in a sense, the crux of applying current-day AI, which can’t understand what’s in your head, but can find a discrepancy in the code you’ve written by aligning it against the nearly uncountable examples its seen and interpreted before.

There are many further uses for AI in the world of software development, some trivial, some staggering (you can see a few such examples here, from OpenAPI themselves) and at least to my perception, it’s approaching a mind-boggling level how straightforward it can be to do something as significant as generating software project infrastructure.

But looping back to where this started, none of these things are any kind of innovation. They’re not even consistent.
 
What, Me Worry?

Perhaps one day we will achieve true AGI, and transcend these bounds. But for now the evidence of all our AI tools — ChatGPT, CoPilot, and so, so many others — points only to them being accessories, if highly capable ones. The insight and built experience of a human remains key; in fact, remains even more key than before, since every human that reviews a AI-generated PR, or evaluates an application developed with the help of AI, has become a gatekeeper, the sole rational determinant of whether the AI speaks truth, or falsehood; whether it has turned out comprehensible, usable code, or glitchy artifacts of strangely shifting matrices of data. And it is in this evaluation that we prove our worth — for the proof is in the pudding, and if the incomprehensible jibberish masquerading as quality code such as only an AI can generate does manage to sneak through this evaluation, and makes it into live applications, we will all know the difference. Let’s just hope we know it right away, when it errors out, and not sometime down the road, when it becomes the Paperclip Maximizer.

When the topic of nutrition arises in conversation, most people will voice an opinion.

Maybe the easy familiarity we have with nutrition comes from it being a close and personal topic, as well as within reach of comprehension, unlike many other medical topics. Regardless of how close our opinions are to the reality, the constantly changing — and very visible — terrain of nutritional science abets our tendency to opine.

It’s well-established: the nutrition industry can’t seem to sit still. Hasn’t for well over a century. And it gives its manifold lectures loudly, through advertisement, marketing, and many other forms.

In any given field, it’s noteworthy how quickly conventional wisdom can change, and how suddenly we discover that something we knew all our lives may be wrong. Many fields have reached this amorphous status, and nutritional science appears to be at the forefront of that paradigm of rapid change — again possibly because it hits so close to home for those interested in improving their well being. Whatever the case, we constantly discover new revelations of how best to handle our nutrition, or learn that a fundamental fact of nutrition was in fact produced by a dubious lobbyist-funded study, or are told that a diet we thought might kill us might instead save us, or vice versa.

It’s all a little hyperbolic, but the underlying truth isn’t — nutrition, and health, and physiology remain evolving fields. The interplay of complex chemicals in a complex organic system, subject to environmental factors and changing lifestyle habits, nutrition, biological adaptation, and many more factors, produces an incredibly complicated and nuanced subject which science still struggles to subdue despite many very real insights and accomplishments.

My own most recent entry point to this chaotic field arose from the ongoing debate around fats, unsaturated fats, Omega-3s especially, and the negative, positive, or indifferent changes they incur in one’s body, depending on who you ask and when you ask them.

It’s all but certain that these chemicals do create changes in our bodies (beyond the obvious digestive and storage facilities) but the “known wisdom,” as it stands, keeps changing. It has been “known” for a reasonable amount of time now that saturated fats are a dangerous substance to put into your body in large quantities — this seems evident, anecdotally, as we can visually point to variances in the human bodies around us that seem to align with this understanding. But then, most of the foods that fall into certain categories will have more than one kind of potentially harmful product, which confounds the variables at play.

If you dig a bit more, you find that many eschew the notion that saturated fats are inherently negative. Just as we know people whose consumption of heavy saturated fats has lead to heart disease or other issues, anecdotally, we also know anecdotally those who never swerved from their dedication to the traditional foods so abundant with these fats, who seem to be doing just fine — thriving, even.

The debate swings around then to unsaturated fats and the potential problems they incur, possibly causing a number of undesirable changes to the body. At this point, many nutritionists and health experts will point to the difference within the category of unsaturated fats, specifically focusing on the health benefits of n-3 (Omega-3) fats as opposed to the possibly inflammatory n-6 PUFA. After all, we all have heard the again conventional wisdom of the benefits of fish oils and the like.

But even this is under debate, and the research keeps evolving. Of particular note is the relationship between n-3 and n-6 PUFA which is most certainly not yet fully understood.

Where does this land us? In a world of uncertainty, it would seem. For me, this shallow dive into fatty acid came via a recent paper discussing the effect of the ratio of n-6 to n-3, which has become a hot topic, of sorts. It’s a subject presenting complex interactions and complex outcomes. It appears challenging to even imagine constructing a stable basis on which to form any level of certainty — but that doesn’t mean we can’t glean some important conclusions, such as the potential harm of too much n-6, or the potential benefit of diets relatively high in n-3, or the moderation of the consumption of saturated fats, just for starters. None of this means we need to take any wild swings in our nutrition. But what we can absolutely do is combine this potential understanding with our own experiences, our own “experimentation,” in a mild way, with the foods that work for us and the foods that don’t. Nutrition is an evolving field, with a lot of experts from a lot of corners. In the end it’s your body and your life on the line, and your responsibility toward yourself.

And why did this obsess me enough to take rumination to postulation? Because the way we examine our own understanding, and the way we make decisions, must be rational and considerate in a time when the body of information is so massive and yet so in motion. It’s important that we embrace this efflux of knowledge into our lives, and learn the skill of integrating it smoothly into our decision processes, without incurring anxiety or skewing too much towards either ignorance or toward baseless dependency on any given self-styled expert.

Without a doubt my limited understanding of this topic, and all topics of healthful nutrition, is in flux. There are some artifacts of knowledge (we can informally call them “good sense”) which seem relatively stable over time (for now) and to which I adhere, and others which I take with more of a (conceptual) grain of salt. As always it’s possible to make a concrete choice in the face of uncertainty and be fairly certain that you’re making the most expedient and plausibly correct choice as possible given the knowledge at hand — certainly a better choice than wholely ignoring the adapting understanding provided by scientific research, even with the knowledge that new research may come around next week which stands our understanding on its head. Both reason and experience tell us that making such tentative choices is often the best course of action — and most often produces a worthwhile result.

In July 1978, the New York Times transitioned from a hot-type to a cold-type system — from cast metal typesetting with red hot metal to the quiet, cool computer-based systems still employed in principle today.

The video exploring the transition between these systems is worth watching if you have the time, for a number of reasons, not the least of which is the view of the modern computer systems put in place.

A number of interesting items jump out in both the old system and the new — including the ferocious speed at which they were able to cast metal — but the most intriguing item was also one of the most minute: a small animation on the new computer systems for the typists. Any time a writer sitting at one of the typist’s terminals finished composing an article, they would press a button to “send” it off to the editor for review. At that point, the article would appear to roll up the screen, curling in at the top as it “left” the typist’s viewscreen and presumably went off to the editor’s terminal.

Of all things, you’d imagine that kind of animation to be a triviality. In a sense, it is — it doesn’t impact the actual production process, nor change the words that are put on the paper. But that fact makes it all the more pertinent in the context of those writers. We’re so adjusted today to minor animations and other decorations used to gloss up the computing systems we use. They’re very easy to add, and half the time we don’t even notice them except when they are absent. But in the ’70s, adding such an animation was no trivial task. The computing power required for that to occur, on each terminal, every time a writer completed a page was an investment, as was the time taken by the system’s programmers to implement it. So why bother? The writers needed to learn a new system anyway; why not let them adjust to thinking about the page they wrote as virtual rather than a physical page delivered to the editor?

There were many adjustments that the writers in that office needed to make: A new keyboard layout, new methods for organizing content on pages, the ins and outs of a software system — for many who had never before touched a computer. Now it would become a part of their daily workflow.

For any typist in that office, the process of turning words into clean, readable, digestible pages of a newspaper was a craft. It’s difficult for us in the age of one-off articles and automated newsletters to think about it this way, but the fundaments of what made a newspaper excel persisted from the old system into the new, and came along with the writers as they transitioned to the new system. It was this sense of artistry and experience that needed preserving in order to achieve success. Not every individual working at the NYT (or other offices, undergoing the same inevitable change) could adjust; it involved a complete readjustment of decades of work experience.

One worker who did make the transition put it so:

    I’ve learned the new stuff, the new processes and all, but I’ve been a printer for twenty-six years…six years apprenticeship; twenty years journeyman — and these are words that aren’t just tossed around. They’ve always meant something to us printers.

    And it’s inevitable that we’re going to go into computers; all the knowledge I’ve acquired over these twenty-six years is all locked up in a little box now called a computer….

Even those who made the transition, who stuck it out to the new system, who effectively threw out years and decades of accumulated knowledge to make way for technology, felt the impact of what was changing.

That one little animation of a page rolling out of the top of the screen bridged this technological gap. It emphasized for the typists that their jobs were still fundamentally the same: You may sit in a different place, type on a keyboard connected to a computer terminal rather than on a mechanical board, the issues you encounter will be electrical rather than mechanical, but at the end of the day the work you do persists for the same purpose, still goes to the same place, still ends in the same goal.

That is the reason for the animation; that is the reason for the time and money spent implementing a minor flourish— to keep the worker in the game, to help them transition, to subtly but clearly remind them that while the system has changed, the crux and value of their work has not.

It’s a valuable lesson for today’s workplace, which is far more transient than most any workplace in the 1970s when this transition and many similar ones took place. When change comes, it comes as much for the worker as for the work itself. A system that changes isn’t just a swapping out of components, or a financial trade-off, but an impact on the nature and culture of a workplace. Recognizing the value of the experience gathered right under your roof produces a transformative impact on the relationship between the organization and the individuals who work there. An investment in the worker is an investment in the business. The profit of loyalty is significant — and it’s a two-way street.

That same worker quoted above, speaking about how the new computer systems replaced the old ways and would do most of the work, brought it together with one pertinent last statement:

    I think probably most jobs are going to end up the same way.
	
In the advancing modern world, it’s not news that the presence of telecommuting, working from home, remote work, or any other description of avoiding the office in the pursuit of work has been on the rise for a number of years. It’s also not news that recent events have caused an unpredicted and rapid rise in this trend — at least temporarily.

Already, many debate whether this forced experiment of sorts has been successful, thereby establishing the notion that work can be done from home in more ways than expected, or whether we are just barely holding it together enough to weather the storm and wait with bated breath for a chance to get back to the office.

Certainly, many businesses now find that they can conduct some or all of their business remotely, with the only novel cost being in frayed nerves derived from trying to figure out unfamiliar tools of communication. The adaptations we’ve been forced towards, willingly or otherwise, have enabled a proving ground for a style of work we’re often too hesitant or time-pressed to examine in any depth voluntarily.

Unsurprisingly, reactions to this recent change vary. Not everyone can expect to adapt to remote work the same way or with the same level of ease, and many factors figure into the level of difficulty. But the found successes, however limited, do serve to enforce the notion that for better or worse, with ease or with difficulty, the deed can be done — remote work is a plausible reality.

But even when successful from an organizational or monetary point of view, the personal side of the problem is of more concern for some: active parents perhaps more than anyone. The argument may be presented that work often demands too great a time investment for someone who is also in the process of rearing a child; the disruptions found in telecommute en masse, among those with children, might reflect a lack of such balance, causing a struggle between having enough time to get work done and time to successfully perform the daily routines of raising a child (or children). Only the artificial differentiation of the office and the resulting unavoidable requirement of external childcare has kept us, in this view, from paying as close attention to this time imbalance in the past.

But there is, in this attitude, a latent assumption that work should make way for life — if balancing the duties of work and child-rearing becomes overwhelming, it is the fault of work for requiring so much of our time. We can approve of this notion conceptually while simultaneously facing the reality that we are paid to work, not to raise a family. And while we could also argue the idea of societal payments toward successful childrearing, we once again face the stark realities of the derivation of value sourcing from direct benefit given to those for whom we produce work, which definitely lies outside of the domain of childrearing, a domain which provides only potential, conceptual benefit to society as a whole and not to any single individual in the here and now.

If we are to address the notion of work-life balance in this way, then, it must be on its own footing — that is, resolving the questions of the relationship between profit and time, and the structure of our day to day lives. How does time spent at work translate into amount of value provided? Does the voluntary nature of childrearing impose its own restrictions on the relatively non-voluntary nature of work? Should the former limitations be encouraged to change or do they reflect a fundamental and natural limit on our choices? Does childrearing constitute a different life paradigm that should bear on how we emphasize the balance of our time?

In any case, it’s all very new. For many people, the entire paradigm of remote work is new. For society as a whole, the changed paradigm of how work gets conducted is new. If we do think of this adoption of remote work as an experiment, it’s one that has so far been far too brief and possesses too many confounding variables to determine a result.

It takes time, often a lot of time, for people to adapt to remote work — this holds true even for industries where telecommute is common. Other industries have been forced to adopt remote work, some for the first time beyond trivial applications, and for individuals in those domains the change is even slower and more challenging.

Months would hardly be enough time to render judgement on whether someone is adapting well to remote work; certainly a period of a few weeks is far too short to reach any conclusions. This is especially true given the abrupt nature of the transition, a disruption which has incurred additional chaos into the transition. It is therefore that much more difficult to say whether mass telecommuting “works.”

But there is one positive of having had such a hasty, rude transition — we often learn best, adapt best, in a crisis. This isn’t a case of a few individuals in an office piloting a program to try out working from home. This isn’t someone discovering they get more work done when they’re at their home office and trying to spend more time there rather than at HQ. This is a case of sudden, massive movements from office to home, entire organizations altering their processes and communications within days. The chaos that this has presented to those attempting to adapt may be offset by the critical lessons which, as a result of the very nature of the problem, get learned fast and hard. When the livelihood not only of yourself, or of your office, but of an entire company, or an entire industry, depends on the successful transition to remote work, we all have a reason to learn and learn fast.

If mass remote work is a success, it may not feel like it — routine and process are too much in flux to determine that right now. If remote work is a failure, hindsight may not even make clear why — there’s too much going on, too many variables that can be conflated and confused. But this transition can, and will, lead to questions. Even if this transition proves temporary, we will have been forced to learn new ways of working, experienced a radically different way of conducting business as a new norm, questioned the whys and hows of processes we’ve often never bothered to question before. And the result of that questioning can only be positive on the sum.

In life, we like to talk about our highs and our lows, the changes and trends, the roller coaster that we live on.

I like to think about these factors as line segments on a graph, and — taking the analogy with a grain of salt — use this approach to think about the quality and progression of my own life.

If we remain cognizant of the limitations of this approach, and the fact that there are many ways to graph such a thing, none of them authoritative, then we gain interesting insights into our own lives, and can take a broader perspective that makes it easier to let the day by day unfold without stressing too much over individual events. We can make changes in life, big or small, in hopes of changing the curve of our lives, and yet not worry too much if things don’t go as planned, since the curve is still being plotted.

Here are some thoughts I keep in mind when it comes to this graphical approach to life’s journey:

    Not every high or low is indicative of a trend. Life has many local minima and maxima — and they’re not all inflection points.
    Understand the fit of the curve to the larger graph of life. A month can be long, but in the course of your life it is a tiny period. Imagine the overall graph.
    Likewise, to stay humble, recognize your curve on the graph that is the totality of history, and observe just how flat it really is.
    That being said, the tiny curves may still have a huge impact on you personally, so recognizing the warning signs for an inflection point is a valuable skill — apply pattern recognition to these instances the same as you would to any other, and keep your algorithm flexible.
    Understand the subjective nature of the subject you are plotting. Your curve looks different from other perspectives. But for your own appreciation, only the inside view counts.
    Minor changes to the value of the independent variable may have unexpectedly large impacts on the output of the function that is your life — no one’s worked out how that particular function works, and everyone’s is different, so take your best shot.
    On the other hand, remain cognizant of the fact that not all input value changes have a visible impact at standard zoom levels. Complex functions have tricky mappings.

Balancing the subjective and objective in a subject as complex as life is no easy matter. Taking the progression of our lives to account in a visual way can make it easier to gain perspective, and hopefully, make better decisions as life marches forward.

A self-driving taxi service, cruise, recently opened its doors in select cities, allowing the average citizen to get a glimpse of an advanced autonomous driving experience that until very recently was limited to those able to buy an expensive car (or willing to rent one just for the fun of it). 

The stated goals of this service include accessibility, mobility, sustainability, and availability -- all aspects which will integrate it well, potentially, into the modern state of intfrastructure. 

Use of this service is early on, though, and we've yet to see what the real impact will be -- or even whether it will see success. 

But the real impact of Cruise is twofold, and isn't any of the explicit targets that Cruise has intended to hit:

First, the cost of use for an autonomous vehicle has the potential to drop precipitously. Self-driving taxi services like Cruise may cut the legs out from under major car manufacturing or even car-share services themselves (which are working towards autonomous rides themselves, but not yet as a central focus). People with no reason to stretch their funds to buy an expensive automated vehicle can easily spend the funds to take a taxi, even one witha a slight premium. Teh change in the understood cost of this technology should not be understated.

Second, it greatly expands the number of people familiar with, and comfortable with, self-driving cars -- increases their popularity, if successful. Again, this is only applicable if successful. But if even a modicum of popular acceptance is rached, the adoptance of self-driving cars as a mode of transportation.
Up until now, the majority of opinion about self-driving cars comes from news sources, ideological pundits (whether for or against the concept, for their own reasons), and occasional word of mouth from the rare person who does own a Tesla or similar vehicle.
If Cruise (or similar services) take off even a little, this *greatly* expands the proportion of people who directly experience the phenomenon of the self-driving car, and have an opinion based on something they have seen and used, rather than on someone else's opinions that have been communicated down the pipeline.
In the best case, this may increase widespread acceptance of self-driving products and services, even causing a spike in consumer demand and increasing marketshare and adoption. On the other hand, if even a few disastrous fatalities take place, the opposite could occur. Even relatively mild negative experiences, such as the taxis not actually ending up where requested, or glitches in the payment processing system leaving taxi-hailers standed and calling on Uber or their ilk, are likely to have an outsize impact on the public interest.

Here's a strategy to help you become more outcome agnostic along the lines of other
things we've discussed.
It's a technique I'm still working on, I only recently realized that I'm doing it,
but I find it pretty useful.
When you start to get worried or concerned about an outcome that you don't like, that
you don't want, first conceptualize the outcome space.
And what I mean by that abstract sounding turn of phrase is simply to place it in the
context of how much it matters and regarding what it matters.
Is this important?
Will it significantly impact my life, short-term, long-term, how much, how little?
And this should be a fairly simple exercise, one done in a split second.
Is it a matter of my health that might have repercussions long-term?
I need to take this very seriously, will this impact my career, but maybe only for a bit?
It's something that I'd really rather not have, perhaps it's a missed job opportunity.
Or something less important, a date that you were going to have and you don't know if the
person will turn up at the blind date.
That's a short-term impact if you don't let it sit on your mind.
And all these things and the way they impact you and the level to which they impact you
and the relationships they have to other parts of your life, you can quickly assess them
and see how serious it is.
And be honest with yourself, because honesty with yourself underlies all success, at least
of this sort.
And then all you need to do is draw a line.
Draw a line separating the things which you can ignore against the things that you cannot
ignore.
The things which will irrevocably, humongously, and importantly affect your life.
And those things which outside of your own feelings in the moment will not actually have
a real long-term effect.
So if you're looking to get a new job, you might say, well I'm very low on funds, I desperately
need a new job.
If I don't get a new job, I won't be able to make rent.
Well then maybe that's on the one side of the line, but if you were just trying to find
a bit of a better job and you don't really like your job or your co-workers, it's long
hours, it's hard.
Maybe you don't have as much money as you'd like, but if you don't get this other job,
it's not going to be the end of the world, if you're being honest.
Well then that's on the other side of the line.
And once you've drawn this line, and it should be in most cases pretty obvious where something
falls.
We'll no doubt recognize that most things in day-to-day life, we hope, fall into the
side of not really mattering as much as we'd like to think they matter.
They impact us much more than they ought to.
That's why we search for outcome agnosticism in the first place.
So once you've placed this, if you have placed this as a non-critical item, all you have
to do is tell yourself, I accept this outcome.
I accept this potential turn of events.
If this is what happens, that's okay.
It doesn't actually affect me, and it won't matter.
Now that sounds like a little magical spell, but the truth is it impacts your consciousness.
The same way that smiling and seeing others smile can make you happy, the same way that
seeing someone yawn can make you tired, the same way that posing in the mirror can make
you feel more confident without getting into the whole controversy of power poses, but
just seeing yourself looking ready can make you feel ready.
All of these kinds of things, telling yourself this doesn't matter, and meaning it, and recognizing
that it doesn't matter, helps it not matter.
So if you were waiting for that blind date to show up, and you were feeling nervous,
and you were feeling self-conscious, and you were feeling like maybe no one could ever
really like you, and if that person was not going to show up, then it would just reinforce
all those negative things that were going on in your head spinning round and round,
and just like that other person didn't show up, and that's probably why the girl from
the bar last week didn't respond to you, and that's probably why you're still not married,
and haven't even had a good relationship, and who knows how long, and probably no one
will ever love you, and you have so many things wrong with you, what you could possibly never
dig yourself out of this hole, and that whole spiral that you are about to launch into.
But all you have to do is recognize, it's just a date, it's episodic, it's counterintuitive
at first, but the less that you can connect these events to the other events of your life,
the more successful they will be. When we talk about athletes, we talk about them being
in the moment, the star of a game, getting the winning shot, or a boxer's knockout punch,
or even just an artist seeing nothing but the painting in front of him that he's working
on, you are in the moment, and what makes you so good at what you're doing is that nothing
else exists but that thing. The artist, the successful artist does not say, boy I hope
this painting turns out well, not till afterwards, he's just painting. If you're lining up for
a three point shot, if you have any chance of making it, you're not scared of missing,
you're not even thinking of missing, you're not even thinking of making it, it's just
you with the ball of the hoop, and you shoot. That's it. It's the same thing here. You don't
think, is my date going to show up? You don't think, why would she, why wouldn't she, did
she come and leave? Am I good enough? Is she good enough? None of this matters. All that
exists is you and the date, and if she shows up, you have some fun. If it goes poorly,
you don't see her again. If it goes great, you do. There'll be plenty of time to think
about it and analyze afterwards if you really want to, and if you really need to. Then and
there, none of that matters. And unless all of your friends swore up and down that they'd
never set you up again after this date, and this is the last date you will ever be on,
then you know what? It really doesn't matter. As long as you remain outcome agnostic. This
date has nothing to do with any of the other dates past or future. All that matters now
is the present. And to achieve that, you just need to tell yourself, I don't really care.
Would I like this date to go well? Sure, I would be happy if this date went well, but
that is post outcome. And that is a critical element. You can say if this date turns out
to have gone well, that would be nice. But it doesn't really matter if it doesn't. And
that shift in attitude can be achieved just by understanding the context and telling yourself
that it does not matter. And that can change the outcome, and that can change your perception
of the outcome. And that can change all the events that come after. But in order to affect
the events that come after, you have to disconnect from those events. And just accept the moment.

One great way to work your way into new areas of thought, or for that matter to
help others to do so, is to start with something that's a little outside of
what you would normally look at, but not very far. You know, book is a good place
to start with that. Because obviously if you pick a book let's say
that is, you know going in that the subject matter is diametrically, or the
point of view is diametrically opposed to what you think, then you can force
yourself to read it, but it won't be a very productive session. Most likely
you'll just be thinking, oh that's wrong, that's stupid, that's dumb, that's wrong,
that's dumb, that's stupid, that's stupid, that's dumb, that's not getting very far
that way, right? Even if you force yourself to get the whole thing. Whereas if
you start, if you start with something that is just a little bit not how you
would think, and you know that, you know, like this is the author coming from a
slightly different viewpoint from you, and you think about it, then you might, you
know, you might still think that, well this thing that they're saying is stupid,
but you, a lot of the time you'll also say, well I see where they're coming from,
I see, I get it, I don't necessarily agree with it a hundred percent, but I get it,
and then you'll have, your brain will have some time to sort of process that, and
then, you know, maybe you'll read a few similar things to that, and then that can
take you to someone who is a little bit farther from where you originally
started, and you can continue on that vein, and before long the things that
seemed so strange won't seem strange anymore, for the most part there are
limitations there. Now you can use this to your detriment too, I mean that's how
a lot of people end up, as we say, going off the deep end, because they get
veered off into the distance by gradually increasing levels of, you know, whatever it is.
But if you're careful about it, and I don't mean careful in the sense of where you go,
because if you have an indention of where you're going, it sort of ruins the whole thing,
but how you do it, that's the important thing. Consciously, cognitively, thinking,
absorbing, reflecting, understanding, filtering through your own understanding
what is true, false, possibly true, possibly false, relevant, you know, open question.
If you do that at every stage, you can wend your way into new topics that might be
forbidding, not because you don't understand them necessarily, but forbidding
because they seem dumb. But there's a great benefit, and I guess a great
adventure in doing that sometimes, you wouldn't want to make it your only kind
of reading, sometimes it's good to delve deeper into the things you already think,
although you should be careful not to get into the trap of just reinforcing things
for the sake of reinforcing them in your own head. Sometimes you can feel good,
but that's not the same thing as learning. But if you want to just dig deeper into a topic
that you already know, this is how you feel, I mean, that makes perfect sense. Why wouldn't
you want to bolster that? But at the same time, it's good to have this other deviant exercise
where you kind of take yourself on this course a new way, and you don't do that by
going diametrically opposite to where you are now, you do that by going around the wheel a little bit.

There is a small, but significant and overlooked phenomenon in education and child rearing,
wherein intelligent children, especially those with relatively high emotional intelligence
and perceptive skills, do not learn right and wrong in the sense that their elders,
teachers, parents, and others think they are.
Instead, they learn expectation management, although they would never put it in those
terms being a young child.
But this phenomenon does occur from a very early age, and its ramifications are outsized,
to say the least, and often very subtle.
This ties in closely to some of Lang's theories in terms of the, I guess, hypnotic manipulation.
Unconscious hypnotic manipulation of those around us, especially children who are in
the learning process and struggling to understand their environments.
If a child has a relatively lax home environment, not unstructured but lax, and so they grow
up with a certain set of rules, and then they go one day to school, to kindergarten, and
they yawn.
But when they yawn, because they're tired nor bored, they don't cover their mouth because
they were never taught to, and the teacher calls them out on it, they say, that's rude.
Don't do that.
And the child is confused, because why would yawning be rude?
From the teacher's point of view, it's very clear that you should at least cover your
mouth the same way as you would with anything else, but the child has never learned this,
has never been yelled at or rebuked for yawning, and even if the teacher now clarifies that
it is not the yawning, rather than not covering one's mouth when one yawns, although this
will quickly be obscured, no doubt, by most likely the teacher's irritation that the child
does appear bored or sleepy in a class where the teacher is no doubt trying their best
to engage the students and views it as a kind of irreverence.
But if we can even get past that in this little interaction, the child is now taught that
to yawn without cover- the child is now taught that to yawn without covering your mouth is
rude.
The child is a little bit confusing, so when he goes home and asks his parents, is it rude?
Because he wants confirmation, so often we go to our parents in that situation.
And they say, um, no, not necessarily, it's not like coughing, you know, you're not coughing
in the most face, obviously that's rude, but yawning isn't rude.
So now, the child is left with a conflating sense of authority, one thing that is expected
at home and a different that is expected in school, now if he is an intrepid child, he
may go back to that teacher and try to speak up and resolve this conflict in his mind and
say, my parents said it wasn't rude to yawn without covering your mouth.
And then the teacher will say something like, well, in my classroom, we don't do that.
Now, those are some magic words right there, do you see it?
The teacher has created a space in which the rules that apply elsewhere do not apply there.
Now this is important training as well for later in life because much of the boundaries
that we impose on ourselves and on others are by space, right?
The way we act at home and the way we act in the supermarket and the way we act in the
office and the way we act on the train are not the same as one another and for good reason,
or at least they shouldn't be, but we haven't given those rules explicitly to the child.
The child has received a conflicting set of rules, one of which was initially put to
him as a universal rule, but now appears to have been scaled back after some resistance
and only applies to this room.
Well does it apply to anywhere else?
There's only one way to find out, but so far no one else has called him out on it.
So now the rule that comes that has been established in his mind is not, it is rude not to cover
your mouth when you yawn, but rather, when I'm around teacher so-and-so, I should not
yawn.
Because presumably he has extrapolated not so much the space, but rather the teacher,
the authority who gave him this rule in the first place, the fact that this was later
dedicated to the space is almost secondary.
So the rule set that's been created here is a management of expectation.
There is not an expectation when he is at home or in the car with his parents that he
should not cover his mouth, that he should cover his mouth, but there is an expectation
of this when he is in the classroom with teacher so-and-so.
Now again, an enterprising child trying to make sense of the world may seek in their
own primitive way to rectify these rules against one another and try to come up with some set
of appropriate actions to try to make it more than just a random collection of appropriate
behaviors, contextually appropriate behaviors and try to make it a little bit more coherent,
a rule set.
But this is almost guaranteed to fail because when you're young especially and either reasons
are not given for the instructions that are given or the reasons are either apparently
nonsensical to the child or at the very least in conflict with one another.
So that the teacher in classroom A doesn't have the exact same set of rules as the teacher
in classroom B and the rules that are given at home by the parents are not the same rules
that are given in the supermarket and if he goes to sleep over at another parent's, another
kid's house, the rules that those parents represent are not the same rules either.
But they're all given with the superficial representation of being assumed rules that
is rules which are naturally the case.
Of course they're this way.
This is how one acts.
And so it becomes almost impossible for the sensing child to construct any kind of coherent
rule structure for themselves.
A less sensitive or a less observant or less emotionally intelligent child may not even
go this far in attempting to construct the rule set.
They will of course build some internal framework for themselves but in some sense it will be
more easy because the ambiguity and bending of the rules from place to place and time
to time and individual to individual will not bother them as much.
They'll just take it as it comes, roll with the punches say.
But a certain kind of child will necessarily want things to make sense.
They're trying to build their own framework for life, again not their words.
What this is, what they're doing.
And the remarkable inconsistency with which the rules seem to spring into existence for
this child and the way they are generally represented which is a truth from above.
And something that the child ought to have already known somehow, which is generally
how it is presented.
Something that it is an enterprise that is doomed from the outset.
All that can be done is to sort of set out a kind of memory map of who, what, where,
when and how one should act.
I'm here with these people therefore I act this way.
I'm saying this thing around this person therefore I act this way.
All to minimize the amount of trouble that one would get in.
Of course some children are more trouble averse than others and some are more willing to get
in trouble but it also depends on the level of punishments and such being dealt and the
severity of the authority.
But regardless to one level or another the child will have learned over the course of
only a few years because by some point they have ossified this internal structure into
something representing an ethical framework and while this can still be added on to and
modified generally at a certain point you'll actually see them get into trouble more because
they have established this kind of convoluted framework for living which they think represents
the amalgamated expectations of the categorizations of the people who have over the last few years
given them rules and so that expectation management kind of morphs into a strange kind of ethical
framework but at the same time you are still left with the residue in the form of a child
who generally will seem fair but often couches their true intents in what they know will
appear to you.
And this is common to some degree in almost all children but it's much more common in
children who go through this process because over that few years they have learned just
this mind map of expectation management and we only exacerbate it generally when we in
a sense validate it by observing it only from the outside that is to say the child acts
around you generally when there is no overriding reasoning like something they really want
or they're just feeling pouty that day but generally they will act around you how they
know you expect them to act in that situation and therefore you will praise them and again
you will say something like good job or I'm so proud of the young man you've turned into
or perhaps worst of all say nothing at all just take it for granted thereby silently
validating that this is the correct course of action.
And so blind either willfully or unwittingly to what is going on in the child's head to
what they are really thinking and what they are doing when you're not around or not watching
you tend to praise the outcome which is inherently praising the framework which only reinforces
this kind of contorted moral I won't say ambiguity but a framework which allows for some very
otherwise seemingly disingenuous actions because where most of the time action A will seem
to contraindicate action B in this child's head they will not do so at all because they're
simply part of the same framework that was developed from this expectation management
something that was good it to one person but bad to another therefore allows something
good and something bad from one person's point of view but it has been validated as a whole
they were praised for a positive action as viewed by one person where they could do if
they could do action A around the one person but not action B and they're praised because
they only do action A and then they go to the second person where they could do action
B but not action C and they do action B but not action C and they're praised for that
now they know they can do all these crosswise kind of actions but as long as they're doing
in the right times and places this is quote good unquote they are still the good child
this is a tricky and dangerous thing and it often results in some very unexpected behaviors
from time to time children like this tend to be the ones who are little angels until
they murder someone or that may be a little bit exaggerated but often it turns out that
they are not only able not only willing but view it as a positive thing to do something
which we societally might consider very very negative even just morally negative even though
it's not a legal thing now you can try to resolve this issue get ahead of it by kind
of confronting it and a pertinent a an attentive parent or role model will ask around and note
the different expectations that different teachers and authority figures and guardians
have and then try to work around this by providing a more firm framework explicitly for the child
but unless you catch this early and synthesize it with the expectations that they will see
in everyday life you can give them you can give them a set of rules and say these supersede
these countermand again not in these words these supersede these countermand anything
that other people tell you but that will only go so far it will be guaranteed to fail as
soon as it breaks down in the face of reality because if you are let's say the parent and
you say well you know your parents rules come first but if you still have to in order to
avoid getting into trouble avoid having it just a miserable day at school or wherever
have to follow the expectations of your teachers each of whom might differ somewhat
you still end up with this framework you have to break those rules that were given to you
in the explicit framework and still develop your implicit framework so it's a tricky thing
and it requires it to resolve it properly requires really having a sense of what your
child is thinking which can be a challenging endeavor at any time and also that when they are
performing an action which you view as good or performing an action which you view as bad
to address the root cause of it to give as much of a reasoning as you can to come not with the
expectation that they should have already known this but the almost questing attitude of why did
this happen and what will we do about it to make sure that you don't or that you continue to do it
in the other hand because making the child feel just a little bit of soul searching will go a lot
farther than either praising them or admonishing them will at least for the majority of cases in this
category.

life, death, honor, dignity, survival.
I am troubled by the question of the human tendency for survival.
There are two main perspectives here.
One, that human life is inherently valuable,
that we as individuals should want to survive
not only out of the raw animal fear of death,
but out of the quality and dignity of our life itself,
that we can exist and do good,
and that while we can sustain ourselves,
we should sustain ourselves.
And that even through trial and tribulation,
even in the darkest of times,
we should make such efforts as are within our power
to ensure that we do come out the other side.
And that if in doing so it requires us to stoop to depth,
which in another lifetime we would have fewn as dirty or disgraceful
or concerning or problematic or embarrassing or painful.
We should do so if we can come out on the other side,
if we have a chance of coming out on the other side.
And that strikes a chord that seems very true,
but it's also very plausible.
It's also very meaningful to say human life is valuable in its dignity.
A life without any dignity is hard to justify.
We have all seen wretches whose lives would better not be lived.
It is not for us to render judgment on how they got there
or whether they should be alive
as long as they are not doing something that judgment of law says,
deserves death and a few things do now.
Then it is not for us to deliver that death,
but certainly there is something in our hearts that says this is wrong.
Can you blame a man for remaining alive by theft,
even murder, by all sorts of dirty deeds?
If no man has directly wronged him,
but only the universe spurned him so that his life fell into such low places
that in order to exist,
it was not only enough to toil,
it was not only enough to struggle,
but first perhaps to beg, then to thieve, then to kill.
Or what about the debasement of one's self?
The other side of that same coin,
perhaps he needn't beg or thieve or kill,
but what if he just needs to place himself in some humiliating position?
A meaningless, mindless job that he has managed to get.
Some labor or other in which those who are far beneath him in dignity,
in theory, wisdom, knowledge, kindness,
are nonetheless those who give him instruction,
those who humiliate him daily.
We see this all the time.
If a man exists for a higher purpose, then certainly we can justify such a thing.
An embarrassing job becomes much less embarrassing
if a man is caring for a family, loved one.
What about himself?
Is it fair to say you should die rather than live an embarrassed life?
Perhaps the embarrassment is only in one's head
or those who are viewing him, and yet,
to what extent is that true?
Should we say, on the other hand,
to this man who is suffering every day,
that he must continue suffering or ought to,
that life is so inherently beautiful,
so inherently wonderful,
that on the off chance that he might somehow, someday against all odds,
be able to dig himself or get dug out of this horrible hole
into which his life has sunk,
should continue living out his miserable existence,
lonely, friendless perhaps,
looked down upon by his co-workers for being in such a mean position,
looked down upon by society for the position he holds,
scraping out just a pittance,
able to afford food but perhaps little else,
the meagerest of housing.
We have social programs in theory to help,
but not everyone benefits from these equally.
Nor is it a guarantee that a man in such a position
would receive the care he needs.
Such care is, after all, impartial,
which is good,
but also means that it does not address the individual's situation.
And if a disability or an infirmity or a mental issue
prohibits him from rising above his current state in life,
and he knows in his heart of hearts things will never change,
but he's young, 20, 30, 40, and has many decades left even of poor living,
is it incumbent on him to suffer those decades?
Should he want to suffer those decades?
I think this is a very tough thing.
We can look at some of the most extreme examples in history,
not seeking to render judgment on the individuals in one way or another
on whom we cannot possibly be fit to render judgment,
having not even touched upon the outskirts of the misery that they have known
and therefore have no right to judge them,
but simply as an exercise to seek to understand and theorize
about the choices that are made and what the correct path for ourselves might be
in situations which might pale in comparison,
and yet analogies can be drawn and lessons as well.
So you think about genocides.
That's an easy one.
One that comes to mind initially is Pol Pot.
Why that one in particular?
Because it was a more passive genocide to a degree.
And while you could equate some of the Russian and Chinese and other genocides
similarly by the intentional famines and such,
this particular social scheme was put into place
with the specific intent of winnowing down to his ideal socialist utopia.
And so the starving worker working in the fields
whose government has explicitly told him,
you do not matter and we do not care if you die.
He labors on nonetheless.
He could run at a soldier and be gunned down.
He could find a way to take his own life and undoubtedly some did,
but most lived until they succumbed to hunger or disease.
I believe some were killed as well, but I believe that was not the majority.
So the point stands that here are however many number of people
were the ones who died of hunger and thirst and disease
by simply living themselves to death under such grueling conditions.
If it took them six months to die in such a way,
was that six months worth living?
If a farmer whose life had been reasonable before
now found that his life was not reasonable at all,
at the outset could see and say that this was unlikely to turn around
and therefore take his own life.
Is that a wise choice?
If you only have one, three, six months to live
and every day of that will be utter brutality, utter misery.
But your life up till now has been reasonable
and you have few significant regrets.
Why not end things?
Is it better to suffer to no end?
It is the human condition to hope to wait that maybe the next day
or the next day or the next day or the next day or the next day
or the next day will bring you some kind of salvation,
but usually it did not.
We can also look at the Holocaust.
There were many of the much more many who died of many who survived.
And often the actions they took to survive were all but unthinkable.
I do not even refer to those who managed to escape or flee
or join the partisans, but those who managed somehow in the ghettos
and the camps to avoid being killed by murderous guards
and to get by on the meager pittance of food and drink.
They were allotted to steal someone they could,
to work for, to do humiliating tasks when the guards would taunt them
with the offer of a little bit more bread, a little bit more soup
if they would do some unspeakable thing.
And for the years of awful, horrendous torture for many of these people,
for most of these people, it didn't matter.
All the horrible things they did, they still died.
All the things they did to try to stay alive to eke out another day
and then die after months or years of suffering.
But some didn't.
Some lived against all odds.
And when the camps were liberated, some went on to live long fruitful lives.
If you are in that moment, day to day, what are you waiting for?
Are you waiting to live or are you waiting to die?
Some saw a third option and we have incidents like the Warsaw ghetto uprising.
If you are to die, die on your feet.
But not everyone can be expected to do this.
How does a very drastic action and demands a special kind of courage?
As does living under such conditions.
As does choosing to die under such conditions.
You have erased and effaced your dignity.
What is left is but the hollow shell of your life.
The hollow shell perhaps of your soul.
Or maybe it's just in hiding until such time as you can get it back.
Later in life when things are different.
If they're different, if you last that long, if you survive that long.
But Gambit, do you take it's easy to say, well, yes, strive for another day.
You're not dead, so you might as well keep fighting.
And at one level, I believe that and it is very easy to see that as a valid point of view.
It's your life, you only get one.
Keep it up if you can manage to eat out one day, do it.
Who knows what will come tomorrow?
And sometimes that's true.
But it is also an unreasonable thing in some sense to demand.
And if you see the abyss in front of you with all but certainty.
Can you not only save your dignity, but in some sense the joy of your life itself
by ending it earlier than it otherwise would.
And in a more pleasant way than it otherwise would.
Seneca said, life is not to be purchased at any price.
No matter how great or well assured certain rewards may be,
I shall not strive to attain them at the price of a shameful confession of weakness.
It is folly to die through fear of dying.
Perhaps it can be summarized.
Perhaps Seneca's view here can be summarized by saying by his quote,
the wise man will live as long as he ought, not as long as he can.
He always reflects concerning the quality and not the quantity of his life.
Well, I am not sure what to think.
Hopefully, for the most part, this is not relevant to most of our lives.
Hopefully things do not get quite so dark.
And if they get so dark, we may have a way to turn it around.
But one never knows what is in store and it is wise to think ahead.
But even outside of these dark thoughts, there are questions that are relevant more broadly
to our society, our time, and our lives.
In every day, we face decisions of quality of life.
We plan for today where we plan ahead.
We make sacrifices for our own current quality of life, or we make sacrifices for our future.
Planning for a hopeful retirement against the job that we have today.
Planning for the future that we give to our children against what we do now.
We have questions of health care, of not only lifespan, but health span.
Into these, the question of quality and quantity and dignity all play.
And there is the question of longevity.
Those who are against longevity research as a whole, or perhaps against the artificial enhancement
of the human lifespan, point out that the brevity of life is its beauty.
That its transience is what lends it value.
And this is not incorrect.
But proponents of longevity will say that immortality is but a byword.
The real gem that we gain from this research is the ability to choose the time and method
of our own death to die with dignity when we have concluded life's affairs successfully.
When we feel that we have lived well and lived enough.
Not when life chooses to snatch it cruelly away.
Not to be wracked in the end by torture, by pain, by disease.
Not to be taken away from our families at an inopportune time.
From this point of view, anti-aging research and the general curing of disease are really
one and the same.
If you ask for true immortality, more ask for the dignity to die at the time of our choosing.
Dignity.
This is the question.
What has more dignity, the length of one's life or the quality of one's life?
The choice to signify the value of one's life by continuing to live against all odds.
Against struggle and trouble and despair in the hope of seeing a better tomorrow.
What to understand if necessary that one has lived as well and as long as one could and
to die with dignity by choice at a time that is appropriate and in a manner that is appropriate.
I do not have an answer for this.
I both envy and despair for those who come to an immediate conclusion about this.
It will be much easier for them to conduct their affairs as they see fit and yet I feel
that in questions such as these, certainty is disillusionment.
Truth is not easy to find and the more certain we are that we have found it, the more we
have fooled ourselves.
The unknowable questions with indefinable answers are what bound humanity in its evolving and
eternal quest for understanding.
We may not answer the questions but the search for the questions themselves provides us with
a kind of answer and a kind of context, a grounding and a philosophy that will guide
us in the difficult decisions of our times and in our own lives.
And there is value in that far greater than being certain ever could.

This is a branch off of the complexity of reality and addressing the complexity of reality.
When you look at something that has occurred that you have done or someone else has done
or just simply has transpired, rarely is there one single reason.
As with confronting the complexity of reality, most people understandably want to keep things
simple and they want to keep things simple to such an extent that they will refuse to
see that which is not simple and reduce by force of will anything that is not simple
into something simple, usually with catastrophic results even if they don't directly observe
those catastrophic results, they will still occur.
This comes up a lot of the time.
If you ever want to test it out, the next time someone asks you why you like something
or why you did something, give them two reasons and watch them pick one, whichever their favorite
is or whichever you slightly indicated is stronger of a cause than the other.
Almost certainly they will boil it down.
Almost certainly they will try to give you one of those causes back as the real cause.
10 times out of 10, that's what occurs and it's very easy to justify because as rare
as it is for there to be only one cause for something, it's equally rare that all the
causes are equally significant but that doesn't make them invalid nor does it mean that you
can ignore them as just one very simple example.
Let's say you went on a cruise for vacation and somebody asked you why a cruise and you
said well I have two reasons, one is it really lets me feel like I'm getting away compared
to other vacations, I'm almost stepping into a different world by going on a cruise and
secondly you know I got a cheap ticket as a discount based on my airline miles and so
someone could easily look at that and say okay well you know cheap ticket that's good
but the real reason you went on a cruise is because you wanted to get that feeling of
getting away, maybe that's true but maybe that's not, maybe you really just happened
to see the opportunity to get a cheap ticket for a cruise, you always thought about going
on a cruise and you like it for the reasons that we were just mentioning so you jumped
on it and it only really came to you later that one of the big benefits that you got
by going on this cruise was the feeling of getting away, in a sense it was the ancillary
cause for going on a vacation, for going on a cruise specifically, the primary cause was
the cheap ticket that popped up but it's not going to look like that and someone is going
to tell you, not ask you but tell you that the real reason you went on a cruise is because
you wanted to get away, not because you wanted a cheap ticket and if you press the point
which will only agitate the person and kind of undermine the conversation about your
vacation, they'll just swap it out, they'll say oh gotcha so you're really just looking
for you know whatever cheap vacation you could find, the rest was just happenstance and look
at that, that's also false, I still have not completely come to a personal understanding
of 100% why this is the case, I can rationalize it as again the desire for simplicity in life
but it really does boggle my mind the extent to which this phenomenon presents itself,
it's almost inevitable that people will tend to grab hold tight of one reason or another
and refuse to consider the fact that there may be two largely equal causes that are the
reasons behind something happening, god forbid you have more than two reasons for doing something,
that's just more than anyone can handle right?
But it's not, in fact almost all of our more interesting decisions and actions are really
precipitated by multiple causes but most of the time a lot of those causes are underlying,
they've been perhaps sitting in the background for long enough that you're not really cognizant
of it, then when a new cause comes and sort of tips the scale causes this thing to occur
if you make this decision or for an event to happen, it will look like that is the cause
for it, that is the reason for it and often it will confuse people because these kind
of slightly more extreme choices and actions often don't look like the sort of thing that
the person would normally do and pretty much any time this occurs if you see someone making
a choice or an action, and this is pretty intuitive right, if you see someone making
a choice or an action that they normally wouldn't do and there's a very obvious reason why they
did it but it still doesn't fit, it doesn't seem right for why that person would do it,
you know there are other causes behind it, some of the time they're just hiding it from
you, it's a rational conscious thing so they tell you one reason but there's another reason
also going on, maybe a more important reason but much more often especially if you're someone
who knows them well and they're not lying to you or hiding something from you, a lot
of the time it was precipitated by other causes, other reasons which they may not even have
been aware of but which have been underlying this change for some time.
This happens a lot in dramatic career moves, that happens, a big one is a midlife crisis,
you see it in New Year's resolutions, you see it in basically any time someone who is
otherwise fairly reasonable and rational takes a drastic move, it's easy to pin it down
and try to say that something just threw them over the edge or went on a whim but a lot of
the time that's not actually what's happening, a lot of the time maybe they're getting fed
up with something, maybe something that a person said to them a while back has been
ruminating in the back of their head, maybe they've slowly been changing their mind on
a subject but not consciously, visually and so this new stimulus has just caused them
to kind of appear to flip-flop instantly or nearly instantly, I'm sure if you think about
it that you can come up with any number of examples for this that you've seen other people
do in your own life or perhaps yourself, a lot of the time we're aware of it if only
after the fact and performing a kind of post-mortem on these actions is actually very useful because
then you can you can determine the causes that come into play in the machinations of
your life and why that is important other than just understanding yourself and understanding
other people who matter to you is seeing in advance actions that were you not looking
at them in this light might appear unpredictable but in fact are very predictable, not everything
is but a number of them are, the better you understand a person the more so it will be,
you'll see someone who's about to fly off the handle or explode or make a strange move
before anyone else does, perhaps before they do, you can reflect this back on yourself
and make more sage decisions for yourself or pause, halt yourself in the process of
taking an action that you might regret later because there was one cause that you were
aware of and one cause that was a little bit more unconscious or subconscious until recently,
the more you're aware of it the more you can take control of it but looking at cause and
effect as though all outcomes have one cause and any other causes are at best tangential
is no more reasonable than trying to solve an equation when you've not yet solved for
its variables and I know there are some situations where you do that so pardon the analogy but
if you have a problem you need to find its factors, if you ignore your factors your solution
will be incorrect and that's very much true for human actions and decisions choices movements,
the more factors you understand and validate in your own head in the sense of recognizing
them and accepting them that they are there and the more accurate that assessment is the
better chance you will have of seeing reality for what it is and understanding why people
do things including yourself and gaining control over those things and being able to inject
yourself into that decision process for yourself and those you care about when we try to simplify
our lives often we do ourselves a disservice there are times when simplification is very
positive but attempting to defraud ourselves out of the true causes for things trying to
pacify our sense of complexity by lopping off the true origins of how a decision was made is no
beneficial thing it may feel like an easier way to think at the moment at the time but it all
but guarantees a poor outcome this is a major player in relationships as well any kind of
relationship I don't mean just marriage dating any kind of association you have a close association
with another human being validly assessing why a person is doing something why that person is
doing something or why you are doing something in relationship to that person is of utmost
importance in relating to them well you know we all talk about honesty and communication and
transparency and these are important factors for sure but they only matter if you're aware of what
those things are you can be as upfront and honest as you like but if you're not giving yourself the
will to actually recognize the factors that are going on you won't be able to communicate them no
matter how willing you are to communicate them and there'll be an elephant in the room that not
only are you not talking about you're not even aware of but on the flip side if you start to
learn to think about it and accept it and see it for what it is understanding that you won't have
100 success no one does but the more you can do this the more things will kind of
sift themselves out and seem to fall into place a little bit again not always but often
the more you can pause think about it the better at it you'll get the more you'll see the moving
parts the more smoothly you'll be able to relate to those around you as well as to yourself
and gain a little bit of mastery over your own life it's just another part of reality
painful to accept oh so powerful.

I used to really appreciate Slack as an app. The engineering seems impressive, but the constant iteration of features and layout changes typifies the "churn" where a perfectly good app could be left perfectly well alone -- and it would be perfect. Yet instead it is endlessly modified and crowded by new "features."

I really wish providing the ability to toggle any new feature off were an industry standard.

I know this has been said before, but...it's nearly 2024 and 99% of major companies, including tech industry, still can't make a mobile-friendly site (or won't)?

The learning curve for this was in ~2010. I have to believe it's largely intentional -- possibly to drive people away from sites and towards apps -- but many of these truly terrible, nearly unusable sites don't offer apps, so it just becomes a major company that can't hack together device compliance. 

Maybe there's more to it?

Of course GPTs do code better than math, and images and written word better than either.

It's so much easier to BS in English than to BS in code, and even more so than in math.

Math either coheres or does not. Code either works or does not (but has fudge factor, sometimes, in interpretation).

Language and art OTOH are imaginative, and accuracy is only one facet. There's wiggle room. 

We bring our human motivations, and the fuzziness of diffusion models takes us for an interpretive ride.

Been exploring the use of AI tools, how they can fit into life organically (I know, same as everyone).
Getting some interesting (to me) insights in the ways that an AI tool can act as labor-saving device, which works best when the AI is not being crammed into a role it doesn't suit.
Example: NovelAI still feels iffy as  writer / storyteller (this seems a common opinion) but finds an excellent niche in breaking me past writer's block, and iterating on ideas -- a companion, not a solution itself.

Many technological advancements prove beneficial. Adopting such improvements is good. Helping others determine how technology may help them also can be good. Conceiving expectations on how others should also adopt these improvements or be left behind by the changing world is bad.
In other words: create change, not breaking changes. Technology, properly developed, influences its adoption naturally. Adoption by coercion generally indicates false advancements. Real technology helps, never hinders.

Engineers, coders, planners and all other detail-oriented professionals often take flak (sometimes rightly so) for hyperfixating on minutiae. 

The question for all detail-oriented thinkers to ask ourselves is "is this detail important (here and now)?"

Sometimes the answer is yes -- and that needs to be given legitimacy.
Sometimes the answer is no -- and we need to learn to let go.

It's a two way street and often it's very difficult for each to understand the other's communication style.

Like so many people, I've been experimenting with the image generation AIs recently made publicly available -- there's an interesting recurrent outcome, especially it seems with topics the AI may understand via fewer representations, that when lacking enough information to fully form a concrete picture, it starts adding extra objects into the scene that relate to the theme of that picture -- ask for an ironsmith and get all kinds of extra tools, ask for a medieval archer and get extra weapons.

An issue I had early on in math, and which I've seen others struggle against, is getting unnerved by the natural abstruseness of symbology. All the δs and ∀s and whatnot make math seem much, much more complex than it often is. 

It's true - the nitty gritty of calculations and derivations can get intense, especially for people (like me) who
started out behind.  

But 9/10 times in the world of math, all you need is to get what all those symbols are trying to say -- which is usually much simpler.

Even the introduction to the Meditations of Marcus Aurelius makes for powerful reading, it stands on its own and makes a useful reference in times of confusion or attritional deviation (that being personal experience).

The alzheimers research scandal indicates most or all alzheimers-targeting drugs are at best ineffective, at worst harmful. 

https://www.science.org/content/blog-post/faked-beta-amyloid-data-what-does-it-mean

Why does it seem that, anecdotally, many of the drugs work just fine? 

I have personally heard *many* reports that the advance of alzheimers slowed with the use of meds.
Are these one and all placebo effect?
Or potentially, there is another factor at play here?
The research, while largely invalidated, may still lie along roughly the right lines.

A lot of back and forth here re DDG: https://news.ycombinator.com/item?id=31490515

The unfortunate but practical truth is that the combination of people who care about privacy AND are invested in technology is relatively small. 

For a given individual who fits these above categories, getting heated over privacy companies being imperfect is, imo, not beneficial. It's understandable to feel "betrayed" but there are more practical ways to defend your privacy, and that bottom line is what deserves the focus.

There are two sides to remote work, and anecdotally I've seen deviations to either one.

This sort of outcome is unfortunate but predictable ---any tech organization has administrative decisions to make, and while there are better and worse ways to handle such situations, and we can critique those approaches, criticizing the outcome itself is questionable, in that the choice is intended to prioritize the organization's own interests.

This is where self-ownership technologies become more relevant.

Life parodies itself...at least the ride has nice views.


